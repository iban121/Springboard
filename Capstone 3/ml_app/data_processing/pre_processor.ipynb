{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06a0b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "import contractions\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f802a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "colloquial_contractions = {'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': 'For What It\\'s Worth',\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you',\n",
    "    'ILU': 'I Love You',\n",
    "    'IMHO': 'In My Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My Ass Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'OMG': 'Oh My God',\n",
    "    'PITA': 'Pain In The Ass',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The Fuck',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait',\n",
    "    '7K': 'Sick:-D Laugher'}\n",
    "def expand_colloquialisms(tweet):\n",
    "    if tweet.upper() in colloquial_contractions.keys():\n",
    "        return colloquial_contractions[tweet.upper()]\n",
    "    else:\n",
    "        return tweet\n",
    "def expand_contractions(tweet):\n",
    "    return contractions.fix(tweet)\n",
    "def demojize(tweet):\n",
    "    emojis = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags = re.UNICODE)\n",
    "    cleaned_text = emojis.sub(r'', tweet)\n",
    "    return cleaned_text\n",
    "def emojis(tweet):\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(':','')\n",
    "    cleaned_text = ' '.join(tweet.split())\n",
    "    return cleaned_text\n",
    "def remove_html_text(tweet):\n",
    "    html_text = re.compile(r'<.*?>|\\n|\\t|\\&\\w*;z|\\$\\w*')\n",
    "    cleaned_text = html_text.sub(r'',tweet)\n",
    "    return cleaned_text\n",
    "def remove_url_text(tweet):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+|http://t.co/|http|https?:\\/\\/.*\\/\\w*')\n",
    "    cleaned_text = url.sub(r'',tweet)\n",
    "    return cleaned_text\n",
    "stop_words = ['abyhrgsss', 'ableg', 'abuses', 'absence', 'accidentally', 'accepte', 'acc', 'abcchicago', 'abstract', 'academia', 'abe', 'accordingly', 'abomination', 'abandon', 'accept', 'abject', 'ability', 'abha', 'about', 'accidentalprophecy', 'abstorm', 'acaciapenn', 'abortions', 'able', 'aattamnmd', 'abandoning', 'accidents', 'abusing', 'aaaaaaallll', 'above', 'accepts', 'abc', 'according', 'aace', 'aashiqui', 'aberdeen', 'aamir', 'abceyewitness', 'accidently', 'abran', 'abq', 'aboard', 'abu', 'absurd', 'abs', 'aal', 'abjabhqhe', 'abandoned', 'aberdeenfanpage', 'abolxmhvy', 'accompanying', 'abcnorio', 'absolutely', 'absolute', 'acarewornheart', 'account', 'aar', 'aberystwyth', 'aaaa', 'abgfglhx', 'abysmaljoiner', 'acbryenuo', 'abia', 'abbswinston', 'aan', 'ablaze', 'abgctvfua', 'abortion', 'abuse', 'accountable', 'abnrcr', 'abomb', 'abnzqwlig', 'aawzxykles', 'aaronthefm', 'access', 'abbott', 'aauizggcq', 'abbyairshow', 'aberdeenfc', 'abandonedpics', 'abcnews', 'abninfvet', 'accidentsua', 'absurdly', 'abnlseqb', 'abouts', 'aba', 'aannnnd', 'aaarrrgghhh', 'accionempresa', 'abused', 'ablzmgzv', 'aadzvsr', 'accident', 'absolut', 'aaemiddle', 'abbandoned', 'aaaaaand', 'abbruchsimulator','10', '11', '12', '13', '14', '15', '16', '163', '17', '18', '19',\n",
    "       '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30',\n",
    "       '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41',\n",
    "       '42', '43', '44', '45', '46', '47', '48', '49', '51', '52', '54',\n",
    "       '55', '56', '57', '58', '60', '61', '62', '65', '66', '71', '74',\n",
    "       '76', '77', '78', '79', '8', '81', '85', '86', '9', '93', '94',\n",
    "       '95', '96', '97', '98', 'actually', 'after', 'again', 'against',\n",
    "       'ago', 'air', 'all', 'almost', 'already', 'also', 'always',\n",
    "       'ambulance', 'america', 'amp', 'and', 'angry', 'annihilated',\n",
    "       'annihilation', 'another', 'any', 'anyone', 'anything',\n",
    "       'apocalypse', 'are', 'area', 'army', 'around', 'arson', 'arsonist',\n",
    "       'ass', 'attack', 'attacked', 'august', 'australia', 'away', 'baby',\n",
    "       'back', 'bad', 'battle', 'beautiful', 'because', 'become', 'been',\n",
    "       'before', 'behind', 'being', 'believe', 'best', 'big', 'bioterror',\n",
    "       'bioterrorism', 'black', 'blast', 'blood', 'bloody', 'boat',\n",
    "       'body', 'bomb', 'bombed', 'both', 'boy', 'breaking', 'brown',\n",
    "       'building', 'buildings', 'burned', 'burning', 'bus', 'bush',\n",
    "       'business', 'but', 'call', 'came', 'can', 'cannot', 'car', 'care',\n",
    "       'casualty', 'catastrophe', 'catastrophic', 'center', 'centre',\n",
    "       'change', 'check', 'chemical', 'chicago', 'child', 'children',\n",
    "       'china', 'city', 'cliff', 'climate', 'collapse', 'collapsed',\n",
    "       'collided', 'collision', 'come', 'coming', 'content', 'control',\n",
    "       'cool', 'could', 'country', 'couple', 'crash', 'crashed', 'crazy',\n",
    "       'cross', 'cyclone', 'daily', 'damage', 'damn', 'danger', 'day',\n",
    "       'days', 'dead', 'deal', 'death', 'deaths', 'deluged', 'demolished',\n",
    "       'demolition', 'derail', 'derailed', 'destroy', 'destroyed',\n",
    "       'destruction', 'detonate', 'detonation', 'devastated',\n",
    "       'devastation', 'did', 'die', 'disaster', 'displaced', 'does',\n",
    "       'doing', 'done', 'down', 'drive', 'drowned', 'drowning', 'during',\n",
    "       'dust', 'electrocuted', 'emergency', 'end', 'engulfed', 'evacuate',\n",
    "       'evacuation', 'even', 'ever', 'every', 'everyone', 'exploded',\n",
    "       'explosion', 'eyewitness', 'face', 'failure', 'fall', 'falling',\n",
    "       'family', 'famine', 'fans', 'far', 'fast', 'fatal', 'fatalities',\n",
    "       'fatality', 'fear', 'fedex', 'feel', 'few', 'fight', 'film',\n",
    "       'find', 'fire', 'fires', 'first', 'flames', 'flood', 'flooding',\n",
    "       'floods', 'food', 'for', 'forest', 'found', 'free', 'from', 'fuck',\n",
    "       'full', 'game', 'get', 'getting', 'girl', 'give', 'god', 'goes',\n",
    "       'going', 'good', 'got', 'government', 'great', 'green', 'group',\n",
    "       'had', 'hail', 'half', 'hand', 'happened', 'hard', 'has', 'hate',\n",
    "       'have', 'having', 'hazard', 'hazardous', 'head', 'health', 'hear',\n",
    "       'heard', 'heart', 'heat', 'help', 'her', 'here', 'high', 'hijack',\n",
    "       'hijacker', 'hijacking', 'him', 'his', 'history', 'hit', 'home',\n",
    "       'hope', 'horrible', 'hostage', 'hostages', 'hot', 'hour', 'hours',\n",
    "       'house', 'how', 'huge', 'hurricane', 'image', 'info', 'injured',\n",
    "       'injuries', 'injury', 'inside', 'instead', 'insurance', 'into',\n",
    "       'iran', 'isis', 'islam', 'its', 'just', 'keep', 'kids', 'kill',\n",
    "       'know', 'lab', 'land', 'landslide', 'large', 'last', 'latest',\n",
    "       'least', 'leave', 'left', 'let', 'life', 'light', 'lightning',\n",
    "       'like', 'liked', 'line', 'literally', 'little', 'live', 'lol',\n",
    "       'london', 'longer', 'look', 'looks', 'lost', 'loud', 'love', 'low',\n",
    "       'mad', 'made', 'make', 'making', 'man', 'many', 'market', 'mass',\n",
    "       'may', 'media', 'men', 'middle', 'military', 'moment', 'money',\n",
    "       'more', 'morning', 'most', 'move', 'movie', 'much', 'mudslide',\n",
    "       'murder', 'murderer', 'must', 'national', 'natural', 'nearby',\n",
    "       'nearly', 'need', 'never', 'new', 'news', 'next', 'night', 'not',\n",
    "       'nothing', 'now', 'nuclear', 'obama', 'off', 'official', 'oil',\n",
    "       'old', 'omg', 'one', 'only', 'order', 'other', 'others', 'our',\n",
    "       'out', 'outside', 'over', 'own', 'pandemonium', 'park', 'part',\n",
    "       'past', 'peace', 'people', 'person', 'phone', 'photo', 'photos',\n",
    "       'pic', 'place', 'plan', 'plans', 'please', 'police', 'poor',\n",
    "       'possible', 'post', 'power', 'ppl', 'pray', 'prebreak', 'problem',\n",
    "       'property', 'public', 'put', 'quarantined', 'rain', 'rainstorm',\n",
    "       'reactor', 'read', 'ready', 'real', 'really', 'reason', 'red',\n",
    "       'reddit', 'refugees', 'released', 'remember', 'rescue', 'rescued',\n",
    "       'responders', 'right', 'riot', 'rioting', 'rise', 'river', 'road',\n",
    "       'rock', 'rubble', 'run', 'running', 'said', 'sandstorm', 'save',\n",
    "       'saw', 'say', 'says', 'school', 'second', 'security', 'see',\n",
    "       'seismic', 'send', 'service', 'services', 'set', 'she', 'ship',\n",
    "       'shit', 'should', 'sign', 'since', 'sinkhole', 'sinking', 'sirens',\n",
    "       'sky', 'smoke', 'snowstorm', 'some', 'someone', 'something',\n",
    "       'sound', 'state', 'stay', 'still', 'stop', 'storm', 'story',\n",
    "       'structural', 'summer', 'sunk', 'support', 'survive', 'survived',\n",
    "       'survivors', 'take', 'taken', 'taking', 'tell', 'terrorism',\n",
    "       'terrorist', 'texas', 'than', 'thanks', 'that', 'the', 'their',\n",
    "       'them', 'then', 'there', 'these', 'they', 'thing', 'things',\n",
    "       'think', 'this', 'those', 'though', 'thought', 'thousands',\n",
    "       'through', 'thunder', 'till', 'time', 'times', 'today', 'told',\n",
    "       'tomorrow', 'tonight', 'too', 'top', 'tornado', 'totally', 'town',\n",
    "       'traffic', 'tragedy', 'train', 'transport', 'trapped', 'trauma',\n",
    "       'truck', 'true', 'truth', 'tsunami', 'twitter', 'two', 'under',\n",
    "       'united', 'until', 'update', 'usa', 'use', 'used', 'very', 'via',\n",
    "       'video', 'view', 'violent', 'wake', 'want', 'war', 'was', 'watch',\n",
    "       'watching', 'water', 'wave', 'waves', 'way', 'weapon', 'weapons',\n",
    "       'weather', 'week', 'well', 'went', 'were', 'what', 'when', 'where',\n",
    "       'which', 'while', 'whirlwind', 'white', 'who', 'whole', 'why',\n",
    "       'wild', 'will', 'wind', 'windstorm', 'with', 'without', 'woman',\n",
    "       'women', 'work', 'world', 'worst', 'would', 'wounded', 'wounds',\n",
    "       'wow', 'wreck', 'yeah', 'year', 'years', 'yet', 'you', 'your',\n",
    "       'youtube', 'zone']\n",
    "def remove_stopwords(tweet):\n",
    "    cleaned_text = ' '.join([word for word in str(tweet).split() if word not in stop_words])\n",
    "    return cleaned_text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6048d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    expand_colloquialisms(tweets) #expand common slang contractions\n",
    "    expand_contractions(tweets)\n",
    "    demojize(tweets)\n",
    "    emojis(tweets)\n",
    "    tweets = re.sub(r'#\\w+','',tweets)\n",
    "    tweets = re.sub(r'@\\w+|@[^\\s]+','',tweets)\n",
    "    remove_html_text(tweets)\n",
    "    remove_url_text(tweets)\n",
    "    tweets = re.sub(r'\\d+', '', tweets)\n",
    "    tweets = tweets.lower()\n",
    "    tweets = re.sub(r\"\\s+\",\" \",tweets).strip()\n",
    "    teeets = tweets.lstrip(' ')\n",
    "    tweets = re.sub(r'\\b\\w{1,2}\\b','',tweets)\n",
    "    tweets = re.sub(r\"\\s+\",\" \",tweets).strip()\n",
    "    remove_stopwords(tweets)\n",
    "    tweeet_tokenizer = TweetTokenizer()\n",
    "    tweets = tweeet_tokenizer.tokenize(tweets)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweets = lemmatize(tweets)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df = 0.25, ngram_range = (1,2))\n",
    "    tweets = tfidf_vectorizer.fit_transform(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78582000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd432eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
